{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "d9LIGg1U6Zcs",
        "kNffu_2_HxSy",
        "RHEBXjluHzc1",
        "VkaRLMHMIKqK",
        "RpXktQOcINfN"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LIGg1U6Zcs"
      },
      "source": [
        "###1) Rode o mesmo programa nos dados contendo anos de escolaridade (primeira coluna) versus salário (segunda coluna). Esse exemplo foi trabalhado em sala de aula em várias ocasiões. Os itens a seguir devem ser respondidos usando esses dados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M15NjfEz8aN0"
      },
      "source": [
        "from numpy import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rc3eTGKV6WYy"
      },
      "source": [
        "datasetCSV = 'https://gist.githubusercontent.com/AramisAraujo/0009d230ee2f9d2199d30663b314ff88/raw/f498619e16a5ebd1b292c554c072e3e7af300f54/CDP%2520regress%25C3%25A3o.csv'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arF7V54F8H9_"
      },
      "source": [
        "def run():\n",
        "  points = genfromtxt(datasetCSV, delimiter=',')\n",
        "  #Hyperparameters\n",
        "  \n",
        "  \n",
        "  #how fast the model learns\n",
        "  learningRate = 0.0001\n",
        "  \n",
        "  # y = mx + b (slope formula)\n",
        "  initialB = 0\n",
        "  initialM = 0\n",
        "  \n",
        "  #How many training step iterations\n",
        "  numIterations = 1000\n",
        "  \n",
        "  [b, m] = gradientDescentRunner(points, initialB, initialM, learningRate, numIterations)\n",
        "  \n",
        "  print(b)\n",
        "  print(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTM9Emmf-IZq"
      },
      "source": [
        "def gradientDescentRunner(points, startingB, startingM, learningRate, numIterations):\n",
        "  # y = Mx + B\n",
        "  b = startingB\n",
        "  m = startingM\n",
        "  \n",
        "  for i in range(numIterations): \n",
        "    b, m = stepGradient(b, m, array(points), learningRate)\n",
        "    \n",
        "  return [b, m]\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPAMUaPz-p5d"
      },
      "source": [
        "def stepGradient(bCurrent, mCurrent, points, learningRate):\n",
        "  #Gradient descent\n",
        "  \n",
        "  bGradient = 0\n",
        "  mGradient = 0\n",
        "  \n",
        "  N = float(len(points))\n",
        "  \n",
        "  for i in range(0, len(points)):\n",
        "    x = points[i, 0]\n",
        "    y = points[i, 1]\n",
        "    \n",
        "    bGradient += -(2/N) * (y - ((mCurrent *x) + bCurrent))\n",
        "    mGradient += -(2/N) * x * (y - ((mCurrent * x) + bCurrent))\n",
        "    \n",
        "  newB = bCurrent - (learningRate * bGradient)\n",
        "  newM = mCurrent - (learningRate * mGradient)\n",
        "  \n",
        "  return [newB, newM]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eybT8tbN-x5T"
      },
      "source": [
        "def computeErrorForGivenPoints(b, m, points):\n",
        "  totalError = 0\n",
        "  \n",
        "  for i in range(0, len(points)):\n",
        "    x = points[i, 0]\n",
        "    y = points[i, 1]\n",
        "    \n",
        "    totalError += (y - (m * x + b)) ** 2\n",
        "    \n",
        "  return totalError / float(len(points))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhzn9rbTF6jQ",
        "outputId": "207e12a1-426a-44f8-f1a5-dab3e7b4973e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "run()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.18234255376510086\n",
            "3.262182267596014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNffu_2_HxSy"
      },
      "source": [
        "###2) Modifique o código original para imprimir o RSS a cada iteração do gradiente descendente. Gere um plot mostrando o RSS por interação.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CNpEzdnXLUQu"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, HTML\n",
        "%matplotlib inline\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Un3sqESIX47"
      },
      "source": [
        "def runShowRSS():\n",
        "  points = genfromtxt(datasetCSV, delimiter=',')\n",
        "  #Hyperparameters\n",
        "  \n",
        "  \n",
        "  #how fast the model learns\n",
        "  learningRate = 0.0001\n",
        "  \n",
        "  # y = mx + b (slope formula)\n",
        "  initialB = 0\n",
        "  initialM = 0\n",
        "  \n",
        "  #How many training step iterations\n",
        "  numIterations = 1000\n",
        "  \n",
        "  [b, m], errors = gradientDescentRunnerRSS(points, initialB, initialM, learningRate, numIterations)\n",
        "  \n",
        "  print(b)\n",
        "  print(m)\n",
        "  return errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQHOyxpPIb4K"
      },
      "source": [
        "def gradientDescentRunnerRSS(points, startingB, startingM, learningRate, numIterations):\n",
        "  # y = Mx + B\n",
        "  b = startingB\n",
        "  m = startingM\n",
        "  \n",
        "\n",
        "  errors = []\n",
        "  for i in range(numIterations): \n",
        "    b, m = stepGradient(b, m, array(points), learningRate)\n",
        "    errors.append(computeErrorForGivenPoints(b, m, points))\n",
        "    \n",
        "  rss = pd.DataFrame()\n",
        "  rss['Error'] = errors\n",
        "  \n",
        "  return [b, m], rss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5I-jOJ6IcA7",
        "outputId": "85fa2d5d-5e3e-412d-8a4c-53fb81514c32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "errors = runShowRSS()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.18234255376510086\n",
            "3.262182267596014\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y5TKEk2ANfLd",
        "outputId": "01659f67-07a9-4db6-e218-23f58acc38f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "display(errors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Error</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2648.238127</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2381.173593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2142.151014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1928.225950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1736.763131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1565.403995</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1412.037629</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1274.774770</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1151.924531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1041.973568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>943.567442</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>855.493931</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>776.668097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>706.118923</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>642.977350</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>586.465569</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>535.887446</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>490.619938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>450.105424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>413.844835</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>381.391502</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>352.345659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>326.349516</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>303.082851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>282.259064</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>263.621651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>246.941039</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>232.011763</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>218.649929</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>206.690955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>103.439016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>971</th>\n",
              "      <td>103.437616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>972</th>\n",
              "      <td>103.436216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>973</th>\n",
              "      <td>103.434816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>974</th>\n",
              "      <td>103.433416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>975</th>\n",
              "      <td>103.432016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>976</th>\n",
              "      <td>103.430616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>977</th>\n",
              "      <td>103.429216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>978</th>\n",
              "      <td>103.427816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>979</th>\n",
              "      <td>103.426416</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>980</th>\n",
              "      <td>103.425016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>981</th>\n",
              "      <td>103.423616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>982</th>\n",
              "      <td>103.422216</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>983</th>\n",
              "      <td>103.420816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>984</th>\n",
              "      <td>103.419417</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>985</th>\n",
              "      <td>103.418017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>986</th>\n",
              "      <td>103.416617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>987</th>\n",
              "      <td>103.415217</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>988</th>\n",
              "      <td>103.413818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>989</th>\n",
              "      <td>103.412418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>990</th>\n",
              "      <td>103.411018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>103.409619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>992</th>\n",
              "      <td>103.408219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>993</th>\n",
              "      <td>103.406820</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>994</th>\n",
              "      <td>103.405420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>103.404021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>103.402621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>103.401222</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>103.399822</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>103.398423</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "           Error\n",
              "0    2648.238127\n",
              "1    2381.173593\n",
              "2    2142.151014\n",
              "3    1928.225950\n",
              "4    1736.763131\n",
              "5    1565.403995\n",
              "6    1412.037629\n",
              "7    1274.774770\n",
              "8    1151.924531\n",
              "9    1041.973568\n",
              "10    943.567442\n",
              "11    855.493931\n",
              "12    776.668097\n",
              "13    706.118923\n",
              "14    642.977350\n",
              "15    586.465569\n",
              "16    535.887446\n",
              "17    490.619938\n",
              "18    450.105424\n",
              "19    413.844835\n",
              "20    381.391502\n",
              "21    352.345659\n",
              "22    326.349516\n",
              "23    303.082851\n",
              "24    282.259064\n",
              "25    263.621651\n",
              "26    246.941039\n",
              "27    232.011763\n",
              "28    218.649929\n",
              "29    206.690955\n",
              "..           ...\n",
              "970   103.439016\n",
              "971   103.437616\n",
              "972   103.436216\n",
              "973   103.434816\n",
              "974   103.433416\n",
              "975   103.432016\n",
              "976   103.430616\n",
              "977   103.429216\n",
              "978   103.427816\n",
              "979   103.426416\n",
              "980   103.425016\n",
              "981   103.423616\n",
              "982   103.422216\n",
              "983   103.420816\n",
              "984   103.419417\n",
              "985   103.418017\n",
              "986   103.416617\n",
              "987   103.415217\n",
              "988   103.413818\n",
              "989   103.412418\n",
              "990   103.411018\n",
              "991   103.409619\n",
              "992   103.408219\n",
              "993   103.406820\n",
              "994   103.405420\n",
              "995   103.404021\n",
              "996   103.402621\n",
              "997   103.401222\n",
              "998   103.399822\n",
              "999   103.398423\n",
              "\n",
              "[1000 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BDfi8yCNict",
        "outputId": "d3773863-f5f3-4322-92aa-057357e9a091",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "figRSS = plt.figure()\n",
        "plt.plot(errors['Error'])\n",
        "figRSS.suptitle('Residual sum of squares over iterations', fontsize=20)\n",
        "plt.xlabel('Iterations', fontsize=18)\n",
        "plt.ylabel('Error', fontsize=18)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Error')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAErCAYAAACGiGfxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncJmZWUdx4DIzs47y/wE9WboL4q7ZsQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHEBXjluHzc1"
      },
      "source": [
        "###3) O que acontece com o RSS ao longo das iterações (aumenta ou diminui) se você usar 1000 iterações e um learning_rate (tamanho do passo do gradiente) de 0.001? Por que você acha que isso acontece?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9uEGm86QKUK"
      },
      "source": [
        "Para os parâmetros learningRate = 0.001 e iterations = 1000, é possível notar o rápido decaimento do RSS. Isto se deu pela adequação dos valores destes parâmetros aos dados e sua distribuição. Nota-se também que a taxa de aprendizado influencia diretamente no decaimento do erro."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkaRLMHMIKqK"
      },
      "source": [
        "###4) Teste valores diferentes do número de iterações e learning_rate até que w0 e w1 sejam aproximadamente iguais a -39 e 5 respectivamente. Reporte os valores do número de iterações e learning_rate usados para atingir esses valores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cfu-qE-abfex"
      },
      "source": [
        "def runParams(learningRate, numIterations):\n",
        "  points = genfromtxt(datasetCSV, delimiter=',')\n",
        "  #Hyperparameters\n",
        "  \n",
        "  \n",
        "  #how fast the model learns\n",
        "  # learningRate = 0.0001\n",
        "  \n",
        "  # y = mx + b (slope formula)\n",
        "  initialB = 0\n",
        "  initialM = 0\n",
        "  \n",
        "  #How many training step iterations\n",
        "  # numIterations = 1000\n",
        "  \n",
        "  [b, m] = gradientDescentRunner(points, initialB, initialM, learningRate, numIterations)\n",
        "  \n",
        "  print('w0: ' + str(b))\n",
        "  print('w1: ' + str(m))\n",
        "  print('error: ' + str(computeErrorForGivenPoints(b, m, points)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YHz1X_xbfah",
        "outputId": "ffbc7c7c-fb29-451c-a4d1-20e97859b4bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "iterations = 100000\n",
        "learningRate = 0.001\n",
        "\n",
        "runParams(learningRate, iterations)\n",
        "print('\\nTarget:\\nw0: -39.0\\nw1: 5.0')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "w0: -39.44332204413956\n",
            "w1: 5.599308181295304\n",
            "error: 29.82881660998503\n",
            "\n",
            "Target:\n",
            "w0: -39.0\n",
            "w1: 5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B6jdtsqbhEv"
      },
      "source": [
        "####Pode-se observar que, experimentando apenas com o número de iterações, é possível atingir os valores esperados para w0 e w1. Neste caso, aumentando o número de iterações para 100000, e consequentemente a precisão do modelo, foi possível atingir estes valores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIrto8B4IMQZ"
      },
      "source": [
        "###5) O algoritmo do vídeo usa o número de iterações como critério de parada. Mude o algoritmo para considerar um critério de tolerância que é comparado ao tamanho do gradiente (como no algoritmo dos slides apresentados em sala). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJnljwTEeJg7"
      },
      "source": [
        "def gradientDescentRunnerTol(points, startingB, startingM, learningRate, tolerance):\n",
        "  # y = Mx + B\n",
        "  b = startingB\n",
        "  m = startingM\n",
        "  \n",
        "  iterations = 0\n",
        "  while computeErrorForGivenPoints(b, m, points) >= tolerance:\n",
        "    b, m = stepGradient(b, m, array(points), learningRate)\n",
        "    iterations += 1\n",
        "\n",
        "  print('iterations: ' + str(iterations))\n",
        "  \n",
        "  return [b, m]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpD9VEqsesH-"
      },
      "source": [
        "def runParamsTol(learningRate, tolerance):\n",
        "  points = genfromtxt(datasetCSV, delimiter=',')\n",
        "  #Hyperparameters : learningRate, tolerance\n",
        "  \n",
        "  \n",
        "  # y = mx + b (slope formula)\n",
        "  initialB = 0\n",
        "  initialM = 0\n",
        "  \n",
        "  \n",
        "  [b, m] = gradientDescentRunnerTol(points, initialB, initialM, learningRate, tolerance)\n",
        "  \n",
        "  print('w0: ' + str(b))\n",
        "  print('w1: ' + str(m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYd4T0K_ezo6",
        "outputId": "a00b3375-d21b-4c00-8c53-0fa33e2f744b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "learningRate = 0.001\n",
        "tolerance = 30\n",
        "runParamsTol(learningRate, tolerance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iterations: 31975\n",
            "w0: -37.55228160346146\n",
            "w1: 5.4867384065436156\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpXktQOcINfN"
      },
      "source": [
        "###6) Ache um valor de tolerância que se aproxime dos valores dos parâmetros do item 4 acima. Que valor foi esse?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3lj3Kp2hljm",
        "outputId": "39cd5324-9f2f-4d3e-aaee-f3a20a3c8f16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "learningRate = 0.001\n",
        "tolerance = 29.83\n",
        "runParamsTol(learningRate, tolerance)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "iterations: 58124\n",
            "w0: -39.28875995068296\n",
            "w1: 5.590107415583852\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48brPv4liuct"
      },
      "source": [
        "####Realizando diversos testes com o valor da tolerância, pude observar resultados similares aos obtidos no item 4 ao utilizar valores de tolerância próximos ao obtido para o erro calculado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "efsoA9N2IOpr"
      },
      "source": [
        "###7) Implemente a forma fechada (equações normais) de calcular os coeficientes de regressão (vide algoritmo nos slides). Compare o tempo de processamento com o gradiente descendente considerando sua solução do item 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f9XPi5UXkaJy"
      },
      "source": [
        "def stepGradientNormal(bCurrent, mCurrent, points, learningRate):\n",
        "  #Gradient descent normal, closed form\n",
        "  \n",
        "  bGradient = 0\n",
        "  mGradient = 0\n",
        "\n",
        "  mGradDivisor = 0\n",
        "  mGradDivider = 0\n",
        "\n",
        "  allFirst = lambda x: [a for (a,b) in x]\n",
        "  allSecond = lambda x: [b for (a,b) in x]\n",
        "\n",
        "  xMean = sum(allFirst(points)) / len(points)\n",
        "  yMean = sum(allSecond(points)) / len(points)\n",
        "  \n",
        "  N = float(len(points))\n",
        "  \n",
        "  for i in range(0, len(points)):\n",
        "    x = points[i, 0]\n",
        "    y = points[i, 1]\n",
        "    \n",
        "    mGradDivider += (x - xMean) * (y - yMean)\n",
        "    mGradDivisor += (x - xMean) ** 2\n",
        "\n",
        "  mGradient = 0 if mGradDivisor == 0 else mGradDivider / mGradDivisor\n",
        "\n",
        "  bGradient = yMean - (mGradient * xMean) \n",
        "    \n",
        "  newB = bCurrent - (learningRate * bGradient)\n",
        "  newM = mCurrent - (learningRate * mGradient)\n",
        "  \n",
        "  return [newB, newM]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjK38i6vkuCK"
      },
      "source": [
        "def gradientDescentRunnerNormal(points, startingB, startingM, learningRate, numIterations):\n",
        "  # y = Mx + B\n",
        "  b = startingB\n",
        "  m = startingM\n",
        "  \n",
        "  for i in range(numIterations): \n",
        "    b, m = stepGradientNormal(b, m, array(points), learningRate)\n",
        "    \n",
        "  return [b, m]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-1fGrqlXBJi"
      },
      "source": [
        "def runStepNormal():\n",
        "  points = genfromtxt(datasetCSV, delimiter=',')\n",
        "  #Hyperparameters\n",
        "  \n",
        "  \n",
        "  #how fast the model learns\n",
        "  learningRate = 0.0001\n",
        "  \n",
        "  # y = mx + b (slope formula)\n",
        "  initialB = 0\n",
        "  initialM = 0\n",
        "  \n",
        "  #How many training step iterations\n",
        "  numIterations = 10000\n",
        "  \n",
        "  [b, m] = gradientDescentRunnerNormal(points, initialB, initialM, learningRate, numIterations)\n",
        "  \n",
        "  print('w0: ' + str(b))\n",
        "  print('w1: ' + str(m))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKlYFfz2XIZ0",
        "outputId": "f8d7ee59-0a01-4724-d2b8-a0e0258665c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "import time\n",
        "\n",
        "print('Running Normalized Coeficients Approach\\n')\n",
        "currentTime = time.time()\n",
        "\n",
        "runStepNormal()\n",
        "runStepNormalTime = time.time() - currentTime\n",
        "\n",
        "learningRate = 0.001\n",
        "tolerance = 29.83\n",
        "\n",
        "print('\\nRunning Tolerance Approach\\n')\n",
        "currentTime = time.time()\n",
        "runParamsTol(learningRate, tolerance)\n",
        "\n",
        "runParamsTolTime = time.time() - currentTime\n",
        "\n",
        "print('\\nNormalized Coeficients Approach time: ' + str(round(runStepNormalTime, 2))+ ' second(s)')\n",
        "print('Tolerance Approach time: ' + str(round(runParamsTolTime, 2))+ ' second(s)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running Normalized Coeficients Approach\n",
            "\n",
            "w0: 39.44625667909923\n",
            "w1: -5.599482874120658\n",
            "\n",
            "Running Tolerance Approach\n",
            "\n",
            "iterations: 58124\n",
            "w0: -39.28875995068296\n",
            "w1: 5.590107415583852\n",
            "\n",
            "Normalized Coeficients Approach time: 1.53 second(s)\n",
            "Tolerance Approach time: 7.15 second(s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rJVpLXUac0i"
      },
      "source": [
        "####Utilizando as equações normais para estimativa dos coeficientes, pudemos obter um tempo de execução consideravelmente menor quando comparado com a abordagem que utiliza um valor de Tolerância para parada do algoritmo."
      ]
    }
  ]
}